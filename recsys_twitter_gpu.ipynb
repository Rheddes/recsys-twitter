{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "recsys-twitter.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "machine_shape": "hm",
   "mount_file_id": "13ihmAn2p_nrH85mv7rXfra37rY8Md0r8",
   "authorship_tag": "ABX9TyN8rzXeCXULN1fdaJog3Ag1",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Rheddes/recsys-twitter/blob/master/recsys_twitter_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSZn8mKIfxjK",
    "colab_type": "text"
   },
   "source": [
    "# Necessary imports & definitions\n",
    "\n",
    "Copy files from drive to local disk, not necessary it is also possible to work directly from drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r1D3hQ7N2MNR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# !cp ./drive/My\\ Drive/RecSys/train_updated.tsv train_updated.tsv\n",
    "# !cp ./drive/My\\ Drive/RecSys/sample.tsv sample.tsv "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqMvNlDcTy-u",
    "colab_type": "text"
   },
   "source": [
    "Set train file variable to correct path"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DHeXqCv8Tmwg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_file = './drive/My Drive/RecSys/sample.tsv'"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKpRWGTETnLR",
    "colab_type": "text"
   },
   "source": [
    "## Install transformers (for BERT models)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6MSNEuZ9vf-W",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!pip install transformers"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWHBKWxKTsU9",
    "colab_type": "text"
   },
   "source": [
    "## Nvidia stats & info"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ET_38Y0Sz0Fv",
    "colab_type": "code",
    "outputId": "62737341-d441-4ad0-f410-80fefd731110",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    }
   },
   "source": [
    "# !nvcc --version\n",
    "!nvidia-smi"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Sun Apr 26 14:59:53 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvX-aYUwTv8C",
    "colab_type": "text"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BVMsiUBbfw14",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from helpers.dataset import MyIterableDataset\n",
    "from helpers.bert_functions import make_bert_model, get_bert_classification_vectors, create_attention_mask_from\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import islice"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU4-Tk9oT-sl",
    "colab_type": "text"
   },
   "source": [
    "## Tell pytorch to use cuda if available"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "15tcccN2yX1l",
    "colab_type": "code",
    "outputId": "036fbaf9-862b-4a3b-8163-eaa650726050",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "use_cuda = True\n",
    "\n",
    "print(\"Cuda is available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"using device: \", device)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Cuda is available:  True\n",
      "using device:  cuda:0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgEcpwUjUCKn",
    "colab_type": "text"
   },
   "source": [
    "## Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_m0j0hpGs4kr",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "f8f84d4d-3bb4-4873-c430-1835154570c0"
   },
   "source": [
    "model = make_bert_model()\n",
    "print('done')"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "done\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI1kmB_af9WF",
    "colab_type": "text"
   },
   "source": [
    "# Read the desired dataset\n",
    "\n",
    "This piece of code can be used to read the desired dataset into memory as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IAAWvn6uadyv",
    "colab_type": "code",
    "outputId": "ed857f5e-c9fb-4579-a3f3-4cc264386f5a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "iterable_dataset = MyIterableDataset('../data/sample.tsv')\n",
    "loader = DataLoader(iterable_dataset, batch_size=12)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "done\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P81_gDcf19wq",
    "colab_type": "text"
   },
   "source": [
    "# Model 1: (distil)BERT\n",
    "\n",
    "This model transform the list of ordered BERT id's in to a feature vector on which we can use regular classfiers (i.e. logistics classifiers, or kNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jANwvEH5VKqL",
    "colab_type": "text"
   },
   "source": [
    "### Clean GPU memory\n",
    "\n",
    "After running the model some things are left in the memory of the GPU this attempts to clean up as much as possible. Certainly not perfect."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i2QqVpklLO5L",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Clean GPU cache\n",
    "if use_cuda:\n",
    "  torch.cuda.empty_cache()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run model on DataLoader (automatically batched)\n",
    "\n",
    "In order to work on larger datasets we can work in batches."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = None\n",
    "labels = None\n",
    "index = 0\n",
    "with torch.no_grad():\n",
    "  if use_cuda:\n",
    "    model.cuda()\n",
    "  for index, batch in islice(loader, 2):\n",
    "    batch_ids = batch[0]\n",
    "    batch_labels = batch[4]\n",
    "    mask = create_attention_mask_from(batch_ids)\n",
    "\n",
    "    if use_cuda:\n",
    "      batch_ids, mask = batch_ids.to(device), mask.to(device)\n",
    "\n",
    "    last_hidden_states = model(batch_ids, attention_mask=mask)\n",
    "    last_features = get_bert_classification_vectors(last_hidden_states)\n",
    "    features = np.concatenate((features, last_features)) if features is not None else last_features\n",
    "    labels = np.concatenate((labels, batch_labels)) if labels is not None else batch_labels\n",
    "    print(index)\n",
    "    index += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(features))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get output from model\n",
    "\n",
    "See if lengths of feature set and labels set match."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(features))\n",
    "print(len(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 2: Logistics classifier\n",
    "We got our output from the BERT model we can now train our logistics classifier to actually classify tweet engagements."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cW1DWJUg3mMe",
    "colab_type": "text"
   },
   "source": [
    "First we split our training set up into train & test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_98YcFo63qIO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neWbHo4c391S",
    "colab_type": "text"
   },
   "source": [
    "Next train our Logistics Classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "btnSZmZg4AFY",
    "colab_type": "code",
    "outputId": "a7728eb1-a784-468e-a665-c40f1468f4f9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    }
   },
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8XutF_H4jdx",
    "colab_type": "text"
   },
   "source": [
    "# Evaluating classifier\n",
    "\n",
    "Now that we have our trained classifier let's see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PpRcU1JB4nyW",
    "colab_type": "code",
    "outputId": "2130a00c-b849-4ebe-a379-e5b4257223aa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZusXK0D4prC",
    "colab_type": "text"
   },
   "source": [
    "Let's compare that to a dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bVtkT5Kk4sBd",
    "colab_type": "code",
    "outputId": "b165992b-d1a5-40f3-cb9b-0cbc26f2bdae",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.600 (+/- 0.75)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTUx_wbt4wpj",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So we currently perform ~10% better than a dummy classifier, awesome."
   ]
  }
 ]
}