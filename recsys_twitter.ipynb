{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recsys-twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "13ihmAn2p_nrH85mv7rXfra37rY8Md0r8",
      "authorship_tag": "ABX9TyN8rzXeCXULN1fdaJog3Ag1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rheddes/recsys-twitter/blob/master/recsys_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZn8mKIfxjK",
        "colab_type": "text"
      },
      "source": [
        "# Necessary imports & definitions\n",
        "\n",
        "Copy files from drive to local disk, not necessary it is also possible to work directly from drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1D3hQ7N2MNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp ./drive/My\\ Drive/RecSys/train_updated.tsv train_updated.tsv\n",
        "# !cp ./drive/My\\ Drive/RecSys/sample.tsv sample.tsv "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqMvNlDcTy-u",
        "colab_type": "text"
      },
      "source": [
        "Set train file variable to correct path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHeXqCv8Tmwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = './drive/My Drive/RecSys/sample.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKpRWGTETnLR",
        "colab_type": "text"
      },
      "source": [
        "## Install transformers (for BERT models)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MSNEuZ9vf-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWHBKWxKTsU9",
        "colab_type": "text"
      },
      "source": [
        "## Nvidia stats & info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET_38Y0Sz0Fv",
        "colab_type": "code",
        "outputId": "62737341-d441-4ad0-f410-80fefd731110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# !nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 26 14:59:53 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvX-aYUwTv8C",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVMsiUBbfw14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "import torch\n",
        "import gc\n",
        "import transformers as ppb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4-Tk9oT-sl",
        "colab_type": "text"
      },
      "source": [
        "## Tell pytorch to use cuda if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15tcccN2yX1l",
        "colab_type": "code",
        "outputId": "036fbaf9-862b-4a3b-8163-eaa650726050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "use_cuda = True\n",
        "\n",
        "print(\"Cuda is available: \", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"using device: \", device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda is available:  True\n",
            "using device:  cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgEcpwUjUCKn",
        "colab_type": "text"
      },
      "source": [
        "## Load pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m0j0hpGs4kr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8f84d4d-3bb4-4873-c430-1835154570c0"
      },
      "source": [
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-multilingual-cased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "# model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-multilingual-cased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights, do_lower_case=False)\n",
        "model = model_class.from_pretrained(\n",
        "    pretrained_weights,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print('done')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI1kmB_af9WF",
        "colab_type": "text"
      },
      "source": [
        "# Read the desired dataset\n",
        "\n",
        "This piece of code can be used to read the desired dataset into memory as a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAAWvn6uadyv",
        "colab_type": "code",
        "outputId": "ed857f5e-c9fb-4579-a3f3-4cc264386f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\n",
        "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\n",
        "                \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\",\n",
        "                \"engaged_with_user_account_creation\", \"enaging_user_id\", \"enaging_user_follower_count\",\n",
        "                \"enaging_user_following_count\",\n",
        "                \"enaging_user_is_verified\", \"enaging_user_account_creation\", \"engagee_follows_engager\",\n",
        "                \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
        "                \n",
        "dataset = pd.read_csv(train_file, delimiter=\"\\x01\", encoding='utf-8', header=None)\n",
        "dataset.columns = all_features\n",
        "print(\"done\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWw6wkfe95Ud",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Parsing text_tokens column into numpy vector and padding them to the same size with zeroes so that they can be imported as tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ8Ygh6Mi01Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset[['text_tokens','like_timestamp']]\n",
        "\n",
        "dataset.text_tokens = dataset.text_tokens.apply(lambda x: np.fromstring(x, dtype=int, sep=\"\\t\"))\n",
        "dataset.like_timestamp = dataset.like_timestamp.apply(lambda x: 0 if math.isnan(x) else 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8M80OsK_aL2",
        "colab_type": "text"
      },
      "source": [
        "Padding text token vectors to same length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuUeAB5P_e3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad59a15f-a4c2-4c05-bdcc-64abfa796aec"
      },
      "source": [
        "max_len = 0\n",
        "for i in dataset.text_tokens.values:\n",
        "  if len(i) > max_len:\n",
        "      max_len = len(i)\n",
        "print(max_len)\n",
        "\n",
        "dataset.text_tokens = [np.concatenate([i, np.zeros(max_len-len(i), dtype=int)]) for i in dataset.text_tokens.values]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjKK3XXx-EhO",
        "colab_type": "text"
      },
      "source": [
        "### Create dataloader\n",
        "\n",
        "We can't use all data at the same time so we have to work in batches. This creates a dataloader that creates batches of 12 rows at once in tensor form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CijmZCdhiOyd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ddbc8f79-123c-4126-b530-254b1f8374a3"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(dataset.text_tokens.values, batch_size=12)\n",
        "\n",
        "# For debugging.\n",
        "for i, batch in enumerate(loader):\n",
        "        print(i, batch)\n",
        "        break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor([[  101, 56898,   137,  ...,     0,     0,     0],\n",
            "        [  101,   137, 74039,  ...,     0,     0,     0],\n",
            "        [  101, 13229, 21885,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 56898,   137,  ...,     0,     0,     0],\n",
            "        [  101,   148, 20024,  ...,     0,     0,     0],\n",
            "        [  101, 12399, 19122,  ...,     0,     0,     0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P81_gDcf19wq",
        "colab_type": "text"
      },
      "source": [
        "# Model 1: (distil)BERT\n",
        "\n",
        "This model transform the list of ordered BERT id's in to a feature vector on which we can use regular classfiers (i.e. logistics classifiers, or kNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BNc2WUaJu3X",
        "colab_type": "text"
      },
      "source": [
        "## Prepare model for distilBERT (if data is not preprocessed)\n",
        "\n",
        "Make small batch set that easily fits in memory, with only necessary data included. It becomes a DataFrame of two columns: `text_token` which contains Numpy arrays of BERT ID's, and `like_timestamp` which contains `1` if liked and `0` if not liked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbK6YXmLVDPO",
        "colab_type": "text"
      },
      "source": [
        "### Create batch\n",
        "\n",
        "In order to not run out of memory we have to work in small batches of the dataset at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F0UcZFK2MH1",
        "colab_type": "code",
        "outputId": "d9d6e3ca-e2b6-4441-cdc2-71d7c359601f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_1 = dataset[:2000]\n",
        "batch_1 = batch_1[['text_tokens','like_timestamp']]\n",
        "\n",
        "batch_1.text_tokens = batch_1.text_tokens.apply(lambda x: np.fromstring(x, dtype=int, sep=\"\\t\"))\n",
        "batch_1.like_timestamp = batch_1.like_timestamp.apply(lambda x: 0 if math.isnan(x) else 1)\n",
        "\n",
        "print(batch_1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     text_tokens  like_timestamp\n",
            "0             []               1\n",
            "1             []               1\n",
            "2             []               1\n",
            "3             []               1\n",
            "4             []               1\n",
            "...          ...             ...\n",
            "1995          []               1\n",
            "1996          []               1\n",
            "1997          []               1\n",
            "1998          []               1\n",
            "1999          []               1\n",
            "\n",
            "[2000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWdXkn1HUvON",
        "colab_type": "text"
      },
      "source": [
        "### Optional\n",
        "\n",
        "Get some stats about the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdrTUz486BUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_1.like_timestamp.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7khOV2kiU2FE",
        "colab_type": "text"
      },
      "source": [
        "### Pad text tokens vector\n",
        "\n",
        "In order for BERT model to work with the dataset we have to pad the input matrix rows to same size. So all `text_tokens` arrays have to be same length. Therefore we first calculate what the maximum vector length is in the `text_tokens` column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQBGeuD1-vQS",
        "colab_type": "code",
        "outputId": "730e7d3c-4392-4dee-dd6e-3eaffc7455ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_len = 0\n",
        "for i in dataset.text_tokens.values:\n",
        "  if len(i) > max_len:\n",
        "      max_len = len(i)\n",
        "      \n",
        "print(max_len)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmZFP8Sw2zNR",
        "colab_type": "text"
      },
      "source": [
        "With the `max_len` we can padd all arrays in `text_tokens` to same length and export it to a 2d numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-yjvAtvSAEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = np.array([np.concatenate([i, np.zeros(max_len-len(i), dtype=int)]) for i in batch_1.text_tokens.values])\n",
        "print(padded)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmgjGYz4U3HK",
        "colab_type": "code",
        "outputId": "ad46b290-075d-4832-c995-568430b32562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(padded).shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 227)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjXazL3AslFK",
        "colab_type": "text"
      },
      "source": [
        "### Masking\n",
        "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTLnOerusJFz",
        "colab_type": "code",
        "outputId": "5c02d5eb-a9fc-4d9d-c3ee-54297000892a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 227)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTS5E4Ey2FfP",
        "colab_type": "text"
      },
      "source": [
        "## Run model to get hidden states\n",
        "\n",
        "Running the output yields a 768 length vector for each row in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jANwvEH5VKqL",
        "colab_type": "text"
      },
      "source": [
        "### Clean GPU memory\n",
        "\n",
        "After running the model some things are left in the memory of the GPU this attempts to clean up as much as possible. Certainly not perfect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2QqVpklLO5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean GPU cache\n",
        "if use_cuda:\n",
        "  del input_ids, attention_mask, model\n",
        "  torch.cuda.empty_cache()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWnqdzz8VTbp",
        "colab_type": "text"
      },
      "source": [
        "### Load tensors and Run\n",
        "\n",
        "This creates the tensors from input data (and sends them to GPU if available) and runs the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6qnZyMKXJrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = torch.tensor(padded)\n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "# Put everything on Cuda if available\n",
        "if use_cuda:\n",
        "  input_ids = input_ids.to(device)\n",
        "  attention_mask = attention_mask.to(device)\n",
        "  model.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "  last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV9OwRE96xSR",
        "colab_type": "text"
      },
      "source": [
        "## Run model on DataLoader (automatically batched)\n",
        "\n",
        "In order to work on larger datasets we can work in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvip_oFH65OI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffba7dca-583c-4d28-ebb6-76588b651cb4"
      },
      "source": [
        "features = None\n",
        "i = 0\n",
        "with torch.no_grad():\n",
        "  if use_cuda:\n",
        "    model.cuda()\n",
        "  for input_tokens_batch in loader:\n",
        "    maxlen = input_tokens_batch.size(1)\n",
        "    mask = torch.arange(maxlen)[None, :] < input_tokens_batch[:, None]\n",
        "\n",
        "    if use_cuda:\n",
        "      input_tokens_batch, mask = input_tokens_batch.to(device), mask.to(device)\n",
        "\n",
        "    last_hidden_states = model(input_tokens_batch, attention_mask=mask)\n",
        "    last_features = last_hidden_states[0][:,0,:].to('cpu').numpy()\n",
        "    features = np.concatenate((features, last_features)) if features is not None else last_features\n",
        "\n",
        "print(len(features))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-988dc72267b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mlast_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlast_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biwWIQyEGZgV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981276f9-0179-45e2-a847-2ce4b5acece4"
      },
      "source": [
        "print(len(features))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "199080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuLtl3EoWEEe",
        "colab_type": "text"
      },
      "source": [
        "### Get output from model\n",
        "\n",
        "Next we have to get the classification features from the output of BERT to a proper matrix. For more details on this see: https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV1k5zrv3HWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0d9b3bc5-eca4-4f91-cdd5-4f263929d0d4"
      },
      "source": [
        "if use_cuda:\n",
        "  features = last_hidden_states[0][:,0,:].to('cpu').numpy()\n",
        "else:\n",
        "  features = last_hidden_states[0][:,0,:].numpy()\n",
        "print(features)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.17406213 -0.04339711 -0.18432969 ...  0.5506897   0.18670759\n",
            "  -0.19857799]\n",
            " [-0.11964405 -0.26719034  0.14500038 ...  0.30625767  0.03193806\n",
            "  -0.04966994]\n",
            " [ 0.10105897 -0.01485022  0.1649243  ...  0.2965125   0.28076914\n",
            "   0.0342547 ]\n",
            " ...\n",
            " [ 0.13603929 -0.33101082  0.10854725 ...  0.39265656  0.3065402\n",
            "  -0.26869738]\n",
            " [-0.0618199   0.24845442  0.43925264 ...  0.26208812  0.10677199\n",
            "  -0.0713095 ]\n",
            " [-0.13499364 -0.0298886  -0.3979845  ...  0.21658844  0.2567397\n",
            "  -0.04231678]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqTXLYmr3RYA",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the feature set, it is good to construct our label set as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCwOMtBv3VZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "626d05d5-c585-4b96-f173-24d0ca468c98"
      },
      "source": [
        "labels = dataset[:12].like_timestamp"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     0\n",
            "1     1\n",
            "2     1\n",
            "3     0\n",
            "4     0\n",
            "5     0\n",
            "6     0\n",
            "7     0\n",
            "8     1\n",
            "9     0\n",
            "10    0\n",
            "11    0\n",
            "Name: like_timestamp, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4kFuFTQBC4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f91003a3-d9dc-4a51-f19a-280290647b3c"
      },
      "source": [
        "print(len(features))\n",
        "print(len(labels))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg98gW2r3kLw",
        "colab_type": "text"
      },
      "source": [
        "# Model 2: Logistics classifier\n",
        "We got our output from the BERT model we can now train our logistics classifier to actually classify tweet engagements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW1DWJUg3mMe",
        "colab_type": "text"
      },
      "source": [
        "First we split our training set up into train & test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_98YcFo63qIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neWbHo4c391S",
        "colab_type": "text"
      },
      "source": [
        "Next train our Logistics Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btnSZmZg4AFY",
        "colab_type": "code",
        "outputId": "a7728eb1-a784-468e-a665-c40f1468f4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(train_features, train_labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8XutF_H4jdx",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating classifier\n",
        "\n",
        "Now that we have our trained classifier let's see how it performs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpRcU1JB4nyW",
        "colab_type": "code",
        "outputId": "2130a00c-b849-4ebe-a379-e5b4257223aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lr_clf.score(test_features, test_labels)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZusXK0D4prC",
        "colab_type": "text"
      },
      "source": [
        "Let's compare that to a dummy classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVtkT5Kk4sBd",
        "colab_type": "code",
        "outputId": "b165992b-d1a5-40f3-cb9b-0cbc26f2bdae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dummy classifier score: 0.600 (+/- 0.75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTUx_wbt4wpj",
        "colab_type": "text"
      },
      "source": [
        "So we currently perform ~10% better than a dummy classifier, awesome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25mXenkYTQ68",
        "colab_type": "text"
      },
      "source": [
        "# Other stuff (utilities)\n",
        "\n",
        "Some random stuff that was useful before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox29IUbZeWbV",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and uploading to GCP\n",
        "\n",
        "Following code blocks define code to download and upload training data to the GCP storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XocUcO2CejPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup GCP credentials\n",
        "project_id = 'positive-nuance-274811'\n",
        "bucket_name = 'recsys-twitter-challenge-us-jku-quarantwits'\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKMYJ3evtWsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download training data\n",
        "!gsutil cp gs://{bucket_name}/train_updated.tsv train_updated.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T6crXiJFwRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload partitioned training data to GCP\n",
        "\n",
        "# !gsutil cp train.001.tsv gs://{bucket_name}/train.001.tsv\n",
        "# !gsutil cp train.002.tsv gs://{bucket_name}/train.002.tsv\n",
        "# !gsutil cp train.003.tsv gs://{bucket_name}/train.003.tsv\n",
        "# !gsutil cp train.004.tsv gs://{bucket_name}/train.004.tsv\n",
        "# !gsutil cp train.005.tsv gs://{bucket_name}/train.005.tsv\n",
        "# !gsutil cp train.006.tsv gs://{bucket_name}/train.006.tsv\n",
        "\n",
        "# !gsutil cp train_updated.tsv gs://{bucket_name}/train_updated.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwd9x6q7e-Te",
        "colab_type": "text"
      },
      "source": [
        "## Download deleted tweet & user ID's\n",
        "\n",
        "Tweets & users can be removed over time and we have to remove the corresponding tweet engagements to adhere to GDPR. This piece of code downloads all these ID's so that they can be removed from our dataset in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_3BInNlNMmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget -O tweetIDs_1.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/01'\n",
        "# !wget -O tweetIDs_2.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/08'\n",
        "# !wget -O tweetIDs_3.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/15'\n",
        "# !wget -O tweetIDs_4.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/22'\n",
        "# !wget -O tweetIDs_5.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/29'\n",
        "# !wget -O tweetIDs_6.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/04/12'\n",
        "\n",
        "# !wget -O userIDs_1.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/01'\n",
        "# !wget -O userIDs_2.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/08'\n",
        "# !wget -O userIDs_3.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/15'\n",
        "# !wget -O userIDs_4.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/22'\n",
        "# !wget -O userIDs_5.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/29'\n",
        "# !wget -O userIDs_6.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/04/12'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BlRacY7DG99",
        "colab_type": "text"
      },
      "source": [
        "## Removing removed user & tweet ID's\n",
        "\n",
        "After getting the removed ID's in the previous step this piece of code removes those ID's from the dataset and stores it as a new one. NOTE: this has to be run manually for each partition currently, by running it 6 times and changing the `00x` in the file names on lines `12` and `25` to the number of the partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU9_NjK6d4iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\n",
        "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\n",
        "                \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\",\n",
        "                \"engaged_with_user_account_creation\", \"enaging_user_id\", \"enaging_user_follower_count\",\n",
        "                \"enaging_user_following_count\",\n",
        "                \"enaging_user_is_verified\", \"enaging_user_account_creation\", \"engagee_follows_engager\",\n",
        "                \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
        "\n",
        "new_dataset = pd.read_csv('./train.006.tsv', delimiter=\"\\x01\", encoding='utf-8', header=None)\n",
        "new_dataset.columns = all_features\n",
        "\n",
        "# print(new_dataset.tweet_id)\n",
        "for i in range(1, 6):\n",
        "  removed_tweet_ids = pd.read_csv(\"tweetIDs_{}.txt\".format(i), header=None)\n",
        "  new_dataset = new_dataset[~new_dataset.tweet_id.isin(removed_tweet_ids[0])]\n",
        "\n",
        "for i in range(1, 6):\n",
        "  removed_user_ids = pd.read_csv(\"userIDs_{}.txt\".format(i), header=None)\n",
        "  new_dataset = new_dataset[~new_dataset.engaged_with_user_id.isin(removed_user_ids[0])]\n",
        "  new_dataset = new_dataset[~new_dataset.enaging_user_id.isin(removed_user_ids[0])]\n",
        "\n",
        "new_dataset.to_csv('./train_updated.006.tsv', sep=\"\\x01\", header=False, index=False, quoting=csv.QUOTE_NONE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSbeP12Jeyia",
        "colab_type": "text"
      },
      "source": [
        "## Unix tools used to split the dataset into partitions\n",
        "\n",
        "In order to have more manageable dataset pieces, they are partitioned into sets of 10GB so that they can be read into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0GAAJMmbj57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !chmod +x split.sh\n",
        "# !apt update && apt install -y bc\n",
        "# !./split.sh\n",
        "# !wc -l train.001.tsv\n",
        "cat train_updated.001.tsv train_updated.002.tsv train_updated.003.tsv train_updated.004.tsv train_updated.005.tsv train_updated.006.tsv > train_updated.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr22G4MmfjNs",
        "colab_type": "text"
      },
      "source": [
        "## Dataset sampler\n",
        "\n",
        "Can be used to create a random sample from a large training set. `sample_frequency` can be defined to what your heart desires."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU8-NRQPfBaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_frequency = 1/6\n",
        "\n",
        "sampled_set = dataset.sample(frac=sample_frequency, random_state=4173141592)\n",
        "sampled_set.to_csv('./sample.tsv', sep=\"\\x01\", header=False, index=False, quoting=csv.QUOTE_NONE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzeUy0MlgXXA",
        "colab_type": "text"
      },
      "source": [
        "## Playground\n",
        "\n",
        "Playground based on the code snippet by the RecSys challenge itself.\n",
        "Can be used to efficiently read the first few lines of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P797Zmg22nbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\n",
        "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\n",
        "                \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\",\n",
        "                \"engaged_with_user_account_creation\", \"enaging_user_id\", \"enaging_user_follower_count\",\n",
        "                \"enaging_user_following_count\",\n",
        "                \"enaging_user_is_verified\", \"enaging_user_account_creation\", \"engagee_follows_engager\"]\n",
        "\n",
        "all_features_to_idx = dict(zip(all_features, range(len(all_features))))\n",
        "labels_to_idx = {\"reply_timestamp\": 20, \"retweet_timestamp\": 21, \"retweet_with_comment_timestamp\": 22,\n",
        "                 \"like_timestamp\": 23}\n",
        "\n",
        "\n",
        "def print_features(features):\n",
        "  print(len(features))\n",
        "  print(features)\n",
        "  print('-----------------')\n",
        "  for feature, idx in all_features_to_idx.items():\n",
        "        print(\"feature {} has value {}\".format(feature, features[idx]))\n",
        "\n",
        "  for label, idx in labels_to_idx.items():\n",
        "        print(\"label {} has value {}\".format(label, features[idx]))\n",
        "\n",
        "with open(train_file, encoding=\"utf-8\") as fileobject:\n",
        "    i = 0\n",
        "    for line in fileobject:\n",
        "        if i == 10:\n",
        "            break\n",
        "        line = line.strip()\n",
        "        features = line.split(\"\\x01\")\n",
        "        i += 1\n",
        "        print_features(features)\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RSN0ioo-Fr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sampled_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByJ3FVnoStcR",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizing text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh77Lru5Itqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['text_tokens'] = dataset['text_tokens'].str.split(\"\\t\")\n",
        "print(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYAmKWu1DasT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['text_tokens'] = dataset['text_tokens'].apply(lambda x: np.fromstring(x, dtype=int, sep=\"\\t\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}