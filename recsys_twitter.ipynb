{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recsys-twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "13ihmAn2p_nrH85mv7rXfra37rY8Md0r8",
      "authorship_tag": "ABX9TyPvMuiFbrFykLRtY1bsiKKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rheddes/recsys-twitter/blob/master/recsys_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZn8mKIfxjK",
        "colab_type": "text"
      },
      "source": [
        "# Necessary imports & definitions\n",
        "\n",
        "Copy files from drive to local disk, not necessary it is also possible to work directly from drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1D3hQ7N2MNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp ./drive/My\\ Drive/RecSys/train_updated.tsv train_updated.tsv\n",
        "# !cp ./drive/My\\ Drive/RecSys/sample.tsv sample.tsv "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqMvNlDcTy-u",
        "colab_type": "text"
      },
      "source": [
        "Set train file variable to correct path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHeXqCv8Tmwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = './drive/My Drive/RecSys/sample.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKpRWGTETnLR",
        "colab_type": "text"
      },
      "source": [
        "## Install transformers (for BERT models)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MSNEuZ9vf-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWHBKWxKTsU9",
        "colab_type": "text"
      },
      "source": [
        "## Nvidia stats & info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET_38Y0Sz0Fv",
        "colab_type": "code",
        "outputId": "6caad2ff-7bfe-4aa2-b520-964837d5624b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# !nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 26 11:53:39 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    33W / 250W |   4543MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvX-aYUwTv8C",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVMsiUBbfw14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "import torch\n",
        "import transformers as ppb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4-Tk9oT-sl",
        "colab_type": "text"
      },
      "source": [
        "## Tell pytorch to use cuda if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15tcccN2yX1l",
        "colab_type": "code",
        "outputId": "d3e8a906-bdd1-4607-f618-e914c3b30451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "use_cuda = True\n",
        "\n",
        "print(\"Cuda is available: \", torch.cuda.is_available())\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  dev = \"cuda:0\"\n",
        "else:\n",
        "  dev = \"cpu\"\n",
        "\n",
        "print(\"using device: \", dev)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda is available:  True\n",
            "using device:  cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgEcpwUjUCKn",
        "colab_type": "text"
      },
      "source": [
        "## Load pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m0j0hpGs4kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-multilingual-cased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "# model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-multilingual-cased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights, do_lower_case=False)\n",
        "model = model_class.from_pretrained(\n",
        "    pretrained_weights,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = True,\n",
        ")\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI1kmB_af9WF",
        "colab_type": "text"
      },
      "source": [
        "# Read the desired dataset\n",
        "\n",
        "This piece of code can be used to read the desired dataset into memory as a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAAWvn6uadyv",
        "colab_type": "code",
        "outputId": "a1e1e6dc-047e-40b6-b6b3-5637f30cebaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\n",
        "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\n",
        "                \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\",\n",
        "                \"engaged_with_user_account_creation\", \"enaging_user_id\", \"enaging_user_follower_count\",\n",
        "                \"enaging_user_following_count\",\n",
        "                \"enaging_user_is_verified\", \"enaging_user_account_creation\", \"engagee_follows_engager\",\n",
        "                \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
        "                \n",
        "dataset = pd.read_csv(train_file, delimiter=\"\\x01\", encoding='utf-8', header=None)\n",
        "dataset.columns = all_features\n",
        "print(type(dataset.text_tokens.values[0][0]))\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P81_gDcf19wq",
        "colab_type": "text"
      },
      "source": [
        "# Model 1: (distil)BERT\n",
        "\n",
        "This model transform the list of ordered BERT id's in to a feature vector on which we can use regular classfiers (i.e. logistics classifiers, or kNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BNc2WUaJu3X",
        "colab_type": "text"
      },
      "source": [
        "## Prepare model for distilBERT\n",
        "\n",
        "Make small batch set that easily fits in memory, with only necessary data included. It becomes a DataFrame of two columns: `text_token` which contains Numpy arrays of BERT ID's, and `like_timestamp` which contains `1` if liked and `0` if not liked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbK6YXmLVDPO",
        "colab_type": "text"
      },
      "source": [
        "### Create batch\n",
        "\n",
        "In order to not run out of memory we have to work in small batches of the dataset at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F0UcZFK2MH1",
        "colab_type": "code",
        "outputId": "5f6c0aa8-d381-487d-e0ce-2d6a9c864bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "batch_1 = dataset[:2000]\n",
        "batch_1 = batch_1[['text_tokens','like_timestamp']]\n",
        "\n",
        "batch_1.text_tokens = batch_1.text_tokens.apply(lambda x: np.fromstring(x, dtype=int, sep=\"\\t\"))\n",
        "batch_1.like_timestamp = batch_1.like_timestamp.apply(lambda x: 0 if math.isnan(x) else 1)\n",
        "\n",
        "print(batch_1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            text_tokens  like_timestamp\n",
            "0     [101, 56898, 137, 33909, 10107, 11490, 10288, ...               0\n",
            "1     [101, 137, 74039, 12436, 12396, 12436, 27746, ...               1\n",
            "2     [101, 13229, 21885, 10681, 10380, 31747, 71309...               1\n",
            "3     [101, 14820, 100, 188, 83279, 10142, 10751, 10...               0\n",
            "4     [101, 56898, 137, 12001, 10731, 20498, 20467, ...               0\n",
            "...                                                 ...             ...\n",
            "1995  [101, 56898, 137, 12895, 11517, 11088, 10627, ...               1\n",
            "1996  [101, 4476, 4348, 1946, 6212, 4522, 5712, 7082...               1\n",
            "1997  [101, 137, 10813, 11447, 13073, 12360, 137, 27...               0\n",
            "1998  [101, 148, 30884, 111806, 111787, 111787, 1117...               1\n",
            "1999  [101, 56898, 137, 24392, 168, 12136, 131, 1401...               1\n",
            "\n",
            "[2000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWdXkn1HUvON",
        "colab_type": "text"
      },
      "source": [
        "### Optional\n",
        "\n",
        "Get some stats about the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdrTUz486BUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_1.like_timestamp.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7khOV2kiU2FE",
        "colab_type": "text"
      },
      "source": [
        "### Pad text tokens vector\n",
        "\n",
        "In order for BERT model to work with the dataset we have to pad the input matrix rows to same size. So all `text_tokens` arrays have to be same length. Therefore we first calculate what the maximum vector length is in the `text_tokens` column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQBGeuD1-vQS",
        "colab_type": "code",
        "outputId": "12643685-55e9-4396-a15b-855558ee95a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_len = 0\n",
        "for i in batch_1.text_tokens.values:\n",
        "  if len(i) > max_len:\n",
        "      max_len = len(i)\n",
        "      \n",
        "print(max_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmZFP8Sw2zNR",
        "colab_type": "text"
      },
      "source": [
        "With the `max_len` we can padd all arrays in `text_tokens` to same length and export it to a 2d numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-yjvAtvSAEO",
        "colab_type": "code",
        "outputId": "95a70f1f-b0e8-4d51-a50f-02f520608963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "padded = np.array([np.concatenate([i, np.zeros(max_len-len(i), dtype=int)]) for i in batch_1.text_tokens.values])\n",
        "print(padded)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  101 56898   137 ...     0     0     0]\n",
            " [  101   137 74039 ...     0     0     0]\n",
            " [  101 13229 21885 ...     0     0     0]\n",
            " ...\n",
            " [  101   137 10813 ...     0     0     0]\n",
            " [  101   148 30884 ...     0     0     0]\n",
            " [  101 56898   137 ...     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmgjGYz4U3HK",
        "colab_type": "code",
        "outputId": "dfc68c4d-4f3f-4ed2-d49e-a0365e9f9708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(padded).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 227)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjXazL3AslFK",
        "colab_type": "text"
      },
      "source": [
        "### Masking\n",
        "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTLnOerusJFz",
        "colab_type": "code",
        "outputId": "1358963e-efce-47e0-ba34-a632128df8ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 227)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTS5E4Ey2FfP",
        "colab_type": "text"
      },
      "source": [
        "## Run model to get hidden states\n",
        "\n",
        "Running the output yields a 768 length vector for each row in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jANwvEH5VKqL",
        "colab_type": "text"
      },
      "source": [
        "### Clean GPU memory\n",
        "\n",
        "After running the model some things are left in the memory of the GPU this attempts to clean up as much as possible. Certainly not perfect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2QqVpklLO5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean GPU cache\n",
        "if use_cuda:\n",
        "  del input_ids, attention_mask, model\n",
        "  torch.cuda.empty_cache()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWnqdzz8VTbp",
        "colab_type": "text"
      },
      "source": [
        "### Load tensors and Run\n",
        "\n",
        "This creates the tensors from input data (and sends them to GPU if available) and runs the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6qnZyMKXJrE",
        "colab_type": "code",
        "outputId": "0a2985e1-d282-4be6-890d-2762add4e7c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "device = torch.device(dev)\n",
        "\n",
        "input_ids = torch.tensor(padded)\n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "# Put everything on Cuda if available\n",
        "if use_cuda:\n",
        "  input_ids = input_ids.to(device)\n",
        "  attention_mask = attention_mask.to(device)\n",
        "  model.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "  last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-e3e34b069439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mtfmr_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfmr_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtfmr_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# Self-Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msa_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msa_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msa_output\u001b[0m  \u001b[0;31m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_per_head\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_reshp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.61 GiB (GPU 0; 15.90 GiB total capacity; 11.71 GiB already allocated; 3.31 GiB free; 11.80 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuLtl3EoWEEe",
        "colab_type": "text"
      },
      "source": [
        "### Get output from model\n",
        "\n",
        "Next we have to get the classification features from the output of BERT to a proper matrix. For more details on this see: https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV1k5zrv3HWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_cuda:\n",
        "  features = last_hidden_states[0][:,0,:].to('cpu').numpy()\n",
        "else:\n",
        "  features = last_hidden_states[0][:,0,:].numpy()\n",
        "print(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqTXLYmr3RYA",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the feature set, it is good to construct our label set as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCwOMtBv3VZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = batch_1.like_timestamp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg98gW2r3kLw",
        "colab_type": "text"
      },
      "source": [
        "# Model 2: Logistics classifier\n",
        "We got our output from the BERT model we can now train our logistics classifier to actually classify tweet engagements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW1DWJUg3mMe",
        "colab_type": "text"
      },
      "source": [
        "First we split our training set up into train & test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_98YcFo63qIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neWbHo4c391S",
        "colab_type": "text"
      },
      "source": [
        "Next train our Logistics Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btnSZmZg4AFY",
        "colab_type": "code",
        "outputId": "88e8eaac-7d1d-4088-ece4-1eec97bc1f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(train_features, train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8XutF_H4jdx",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating classifier\n",
        "\n",
        "Now that we have our trained classifier let's see how it performs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpRcU1JB4nyW",
        "colab_type": "code",
        "outputId": "857f5edc-0d64-4833-92bf-f0e9c14a7db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lr_clf.score(test_features, test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.62"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZusXK0D4prC",
        "colab_type": "text"
      },
      "source": [
        "Let's compare that to a dummy classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVtkT5Kk4sBd",
        "colab_type": "code",
        "outputId": "82e99553-ccb3-4be9-cf4e-57327ba3b419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dummy classifier score: 0.513 (+/- 0.08)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTUx_wbt4wpj",
        "colab_type": "text"
      },
      "source": [
        "So we currently perform ~10% better than a dummy classifier, awesome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25mXenkYTQ68",
        "colab_type": "text"
      },
      "source": [
        "# Other stuff (utilities)\n",
        "\n",
        "Some random stuff that was useful before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox29IUbZeWbV",
        "colab_type": "text"
      },
      "source": [
        "## Downloading and uploading to GCP\n",
        "\n",
        "Following code blocks define code to download and upload training data to the GCP storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XocUcO2CejPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup GCP credentials\n",
        "project_id = 'positive-nuance-274811'\n",
        "bucket_name = 'recsys-twitter-challenge-us-jku-quarantwits'\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKMYJ3evtWsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download training data\n",
        "!gsutil cp gs://{bucket_name}/train_updated.tsv train_updated.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T6crXiJFwRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload partitioned training data to GCP\n",
        "\n",
        "# !gsutil cp train.001.tsv gs://{bucket_name}/train.001.tsv\n",
        "# !gsutil cp train.002.tsv gs://{bucket_name}/train.002.tsv\n",
        "# !gsutil cp train.003.tsv gs://{bucket_name}/train.003.tsv\n",
        "# !gsutil cp train.004.tsv gs://{bucket_name}/train.004.tsv\n",
        "# !gsutil cp train.005.tsv gs://{bucket_name}/train.005.tsv\n",
        "# !gsutil cp train.006.tsv gs://{bucket_name}/train.006.tsv\n",
        "\n",
        "# !gsutil cp train_updated.tsv gs://{bucket_name}/train_updated.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwd9x6q7e-Te",
        "colab_type": "text"
      },
      "source": [
        "## Download deleted tweet & user ID's\n",
        "\n",
        "Tweets & users can be removed over time and we have to remove the corresponding tweet engagements to adhere to GDPR. This piece of code downloads all these ID's so that they can be removed from our dataset in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_3BInNlNMmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget -O tweetIDs_1.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/01'\n",
        "# !wget -O tweetIDs_2.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/08'\n",
        "# !wget -O tweetIDs_3.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/15'\n",
        "# !wget -O tweetIDs_4.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/22'\n",
        "# !wget -O tweetIDs_5.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/03/29'\n",
        "# !wget -O tweetIDs_6.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_engaged_with_tweet_id/2020/04/12'\n",
        "\n",
        "# !wget -O userIDs_1.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/01'\n",
        "# !wget -O userIDs_2.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/08'\n",
        "# !wget -O userIDs_3.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/15'\n",
        "# !wget -O userIDs_4.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/22'\n",
        "# !wget -O userIDs_5.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/03/29'\n",
        "# !wget -O userIDs_6.txt 'https://elasticbeanstalk-us-west-2-800068098556.s3-us-west-2.amazonaws.com/challenge-website/public_data/training/diffs/tsv_deleted_user_id/2020/04/12'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BlRacY7DG99",
        "colab_type": "text"
      },
      "source": [
        "## Removing removed user & tweet ID's\n",
        "\n",
        "After getting the removed ID's in the previous step this piece of code removes those ID's from the dataset and stores it as a new one. NOTE: this has to be run manually for each partition currently, by running it 6 times and changing the `00x` in the file names on lines `12` and `25` to the number of the partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU9_NjK6d4iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\n",
        "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\n",
        "                \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\",\n",
        "                \"engaged_with_user_account_creation\", \"enaging_user_id\", \"enaging_user_follower_count\",\n",
        "                \"enaging_user_following_count\",\n",
        "                \"enaging_user_is_verified\", \"enaging_user_account_creation\", \"engagee_follows_engager\",\n",
        "                \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
        "\n",
        "new_dataset = pd.read_csv('./train.006.tsv', delimiter=\"\\x01\", encoding='utf-8', header=None)\n",
        "new_dataset.columns = all_features\n",
        "\n",
        "# print(new_dataset.tweet_id)\n",
        "for i in range(1, 6):\n",
        "  removed_tweet_ids = pd.read_csv(\"tweetIDs_{}.txt\".format(i), header=None)\n",
        "  new_dataset = new_dataset[~new_dataset.tweet_id.isin(removed_tweet_ids[0])]\n",
        "\n",
        "for i in range(1, 6):\n",
        "  removed_user_ids = pd.read_csv(\"userIDs_{}.txt\".format(i), header=None)\n",
        "  new_dataset = new_dataset[~new_dataset.engaged_with_user_id.isin(removed_user_ids[0])]\n",
        "  new_dataset = new_dataset[~new_dataset.enaging_user_id.isin(removed_user_ids[0])]\n",
        "\n",
        "new_dataset.to_csv('./train_updated.006.tsv', sep=\"\\x01\", header=False, index=False, quoting=csv.QUOTE_NONE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSbeP12Jeyia",
        "colab_type": "text"
      },
      "source": [
        "## Unix tools used to split the dataset into partitions\n",
        "\n",
        "In order to have more manageable dataset pieces, they are partitioned into sets of 10GB so that they can be read into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0GAAJMmbj57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !chmod +x split.sh\n",
        "# !apt update && apt install -y bc\n",
        "# !./split.sh\n",
        "# !wc -l train.001.tsv\n",
        "cat train_updated.001.tsv train_updated.002.tsv train_updated.003.tsv train_updated.004.tsv train_updated.005.tsv train_updated.006.tsv > train_updated.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr22G4MmfjNs",
        "colab_type": "text"
      },
      "source": [
        "## Dataset sampler\n",
        "\n",
        "Can be used to create a random sample from a large training set. `sample_frequency` can be defined to what your heart desires."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU8-NRQPfBaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_frequency = 1/6\n",
        "\n",
        "sampled_set = dataset.sample(frac=sample_frequency, random_state=4173141592)\n",
        "sampled_set.to_csv('./sample.tsv', sep=\"\\x01\", header=False, index=False, quoting=csv.QUOTE_NONE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzeUy0MlgXXA",
        "colab_type": "text"
      },
      "source": [
        "## Playground\n",
        "\n",
        "Playground based on the code snippet by the RecSys challenge itself.\n",
        "Can be used to efficiently read the first few lines of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P797Zmg22nbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\n",
        "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\n",
        "                \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\",\n",
        "                \"engaged_with_user_account_creation\", \"enaging_user_id\", \"enaging_user_follower_count\",\n",
        "                \"enaging_user_following_count\",\n",
        "                \"enaging_user_is_verified\", \"enaging_user_account_creation\", \"engagee_follows_engager\"]\n",
        "\n",
        "all_features_to_idx = dict(zip(all_features, range(len(all_features))))\n",
        "labels_to_idx = {\"reply_timestamp\": 20, \"retweet_timestamp\": 21, \"retweet_with_comment_timestamp\": 22,\n",
        "                 \"like_timestamp\": 23}\n",
        "\n",
        "\n",
        "def print_features(features):\n",
        "  print(len(features))\n",
        "  print(features)\n",
        "  print('-----------------')\n",
        "  for feature, idx in all_features_to_idx.items():\n",
        "        print(\"feature {} has value {}\".format(feature, features[idx]))\n",
        "\n",
        "  for label, idx in labels_to_idx.items():\n",
        "        print(\"label {} has value {}\".format(label, features[idx]))\n",
        "\n",
        "with open(train_file, encoding=\"utf-8\") as fileobject:\n",
        "    i = 0\n",
        "    for line in fileobject:\n",
        "        if i == 10:\n",
        "            break\n",
        "        line = line.strip()\n",
        "        features = line.split(\"\\x01\")\n",
        "        i += 1\n",
        "        print_features(features)\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RSN0ioo-Fr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sampled_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByJ3FVnoStcR",
        "colab_type": "text"
      },
      "source": [
        "## Vectorizing text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh77Lru5Itqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['text_tokens'] = dataset['text_tokens'].str.split(\"\\t\")\n",
        "print(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYAmKWu1DasT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['text_tokens'] = dataset['text_tokens'].apply(lambda x: np.fromstring(x, dtype=int, sep=\"\\t\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}