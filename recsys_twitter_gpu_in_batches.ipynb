{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "recsys-twitter-gpu-in-batches.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-cb5e1467",
   "language": "python",
   "display_name": "PyCharm (recsys-twitter)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/Rheddes/recsys-twitter/blob/master/recsys_twitter_gpu_in_batches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Necessary imports & definitions\n",
    "\n",
    "Copy files from drive to local disk, not necessary it is also possible to work directly from drive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get validation set\n",
    "\n",
    "Get the validation/prediction set from the challenge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -O val.tsv \"https://elasticbeanstalk-us-west-2-800068098556.s3.amazonaws.com/challenge-website/public_data/val.tsv?AWSAccessKeyId=AKIA3UR6GLH6F73MJVWF&Signature=uURRfbcpN3%2BW7tWrUaL6Av8ZX5c%3D&Expires=1588061161\"\n",
    "!cp val.tsv ./drive/My\\ Drive/RecSys/val.tsv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !cp ./drive/My\\ Drive/RecSys/train_updated.tsv train_updated.tsv\n",
    "# !cp ./drive/My\\ Drive/RecSys/sample.tsv sample.tsv \n",
    "# !cp bert_classification_features.csv  ./drive/My\\ Drive/RecSys/bert_22500.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set correct batch index\n",
    "\n",
    "In order to circumvent runtime timeouts, we process the dataset in batches.\n",
    "The validation set has been split up in to 4 different files:\n",
    "\n",
    "```\n",
    "./drive/My Drive/RecSys/val{1..4}.tsv\n",
    "```\n",
    "\n",
    "So we need to run the notebook essentially 6 times, each time changing the `TRANSFORM_ITERATION` constant."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "TRANSFORM_ITERATION=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_file = './drive/My Drive/RecSys/val.{}.tsv'.format(TRANSFORM_ITERATION)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install transformers (for BERT models)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install helpers from GitHub\n",
    "\n",
    "To simplify this notebook several helper functions have been abstracted to separate python files in the git repo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!rm -rf recsys-twitter helpers\n",
    "!git clone https://github.com/Rheddes/recsys-twitter.git\n",
    "!cp -r recsys-twitter/helpers helpers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nvidia stats & info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !nvcc --version\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "from helpers.dataset import PredictionDataset\n",
    "from helpers.bert_functions import make_bert_model, get_bert_classification_vectors, create_attention_mask_from\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tell pytorch to use cuda if available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "print(\"Cuda is available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"using device: \", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load pretrained models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = make_bert_model()\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read the desired dataset\n",
    "\n",
    "This piece of code can be used to create dataset and loader objects which allow stream reading the dataset, as to not occupy to much memory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create dataset\n",
    "\n",
    "Custom dataset type to iterate throught the training file, also performs some preprocessing (see `helpers/dataset.py` for details)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iterable_dataset = PredictionDataset(pred_file, 512)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read dataset with pandas\n",
    "\n",
    "As to read out the tweet_id & engaging_user_id as to form a primary key for every record."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\n",
    "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\n",
    "                \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\",\n",
    "                \"engaged_with_user_account_creation\", \"enaging_user_id\", \"enaging_user_follower_count\",\n",
    "                \"enaging_user_following_count\",\n",
    "                \"enaging_user_is_verified\", \"enaging_user_account_creation\", \"engagee_follows_engager\"]\n",
    "selected_features = ['tweet_id', 'enaging_user_id']\n",
    "unused_features = list(set(all_features) - set(selected_features))\n",
    "\n",
    "validation = pd.read_csv(pred_file, header=None, sep=\"\\x01\")\n",
    "validation.columns = all_features\n",
    "\n",
    "for unused_feature in unused_features:\n",
    "  del validation[unused_feature]\n",
    "gc.collect()\n",
    "\n",
    "np_tweet_ids = validation['tweet_id'].to_numpy()\n",
    "np_engaging_ids = validation['enaging_user_id'].to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create loader\n",
    "\n",
    "The loader reads batches from the dataset and outputs it as an iterable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loader = DataLoader(iterable_dataset, batch_size=150)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1: (distil)BERT\n",
    "\n",
    "This model transform the list of ordered BERT id's in to a feature vector on which we can use regular classfiers (i.e. logistics classifiers, or kNN)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean GPU memory\n",
    "\n",
    "After running the model some things are left in the memory of the GPU this attempts to clean up as much as possible. Certainly not perfect."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean GPU cache\n",
    "if use_cuda:\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run model on DataLoader (automatically batched)\n",
    "\n",
    "In order to work on larger datasets we can work in batches.\n",
    "\n",
    "Indices:\n",
    "```\n",
    "TOKENS_INDEX = 0\n",
    "REPLIED_INDEX = 1\n",
    "RETWEETED_INDEX = 2\n",
    "RETWEETED_WITH_COMMENT_INDEX = 3\n",
    "LIKE_INDEX = 4\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = None\n",
    "labels = None\n",
    "with torch.no_grad():\n",
    "  if use_cuda:\n",
    "    model.cuda()\n",
    "  for batch in tqdm(loader):\n",
    "    batch_ids = batch[0]    # Input text_tokens\n",
    "    mask = create_attention_mask_from(batch_ids)\n",
    "\n",
    "    if use_cuda:\n",
    "      batch_ids, mask = batch_ids.to(device), mask.to(device)\n",
    "\n",
    "    last_hidden_states = model(batch_ids, attention_mask=mask)\n",
    "    last_features = get_bert_classification_vectors(last_hidden_states, use_cuda)\n",
    "\n",
    "    features = np.concatenate((features, last_features)) if features is not None else last_features\n",
    "    # print(\"one iteration done\")\n",
    "\n",
    "export_features = np.c_[np_tweet_ids, np_engaging_ids, features]\n",
    "\n",
    "pickle.dump(export_features, open('bert_classification_val.p', 'rb'))\n",
    "!cp bert_classification_val.p  ./drive/My\\ Drive/RecSys/bert_classification_val.p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Done transforming data\n",
    "\n",
    "Done for now, the generated features can be easily loaded in to other models."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}